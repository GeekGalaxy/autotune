#+TITLE: Acksin Autotune
#+AUTHOR: Acksin
#+OPTIONS: html-postamble:nil body-only: t

#+begin_quote

#+end_quote

* Introduction

Acksin Autotune makes sure that your Linux servers are utilized as
efficiently as possible. It does this by tuning your Linux kernel
based on various criteria. The type of application profile, memory,
CPUs, instance type to bring out the best performance for your
machine.

* Usage

** Help
#+begin_src sh
autotune --help
#+end_src

** List Signatures

#+begin_src sh :results output code :exports both
autotune list
#+end_src

#+RESULTS:
#+BEGIN_SRC sh
{
  "Open": [
    "apache",
    "golang",
    "haproxy",
    "java",
    "networking",
    "nginx",
    "nodejs",
    "postgresql"
  ],
  "Pro": null
}
#+END_SRC


** Tuning

*** Show Signature

#+begin_src sh :results output code :exports both
autotune sig golang
#+end_src

#+RESULTS:
#+BEGIN_SRC sh
{
  "Name": "golang",
  "Description": "Configuration for high throughput Golang apps",
  "Documentation": "Linux Optimizations for High Throughput Golang Apps\n\nGo applications have unique characteristics which require certain\nLinux kernel tuning to achieve high throughput.\n\nGo's Utilization Profile\n\nCPU will not be a bottleneck with Golang applications. Our research\nshows that applications, even those that utilize CGO, do no see CPU be\na bottleneck. The places where performance become bottlenecks are the\nfollowing:\n\n - Garbage Collection\n - Default ulimits\n - Networking\n\nAssumptions\n\nWe will be under the assumption that there will be one primary Go\napplication running on the machine and can have access to all of the\nresources. We also assume that we want high network throughput as the\ngoal is to have high response rate. We want to be able to handle\nmillions of requests.\n\nGC Optimizations\n\nFor all intents and purposes we should be able to increase the GOGC to\na number based on the size of the machine. If I am using a m4.large\ninstance on Amazon I use GOGC=10000. The higher the GOGC value the\nless frequent the Garbage Collection will run. Further, since we are\noptimizing the server to be heavily utilized for a primary Golang\nservice we want to use up all the RAM available to us.\n\nUlimits\n\nUlimits are a security mechanism in POSIX based systems which gives\neach user a certain amount of allocation of various\nresources. However, the resource we are concerned with is file\ndescriptors. (ulimit -n) Since a file descriptor can be a file or a\nsocket we can quickly saturate how many connections an app not running\nas root can use. Further, the default open files ulimit on an Ubuntu\nServer 14.04 are ridiculously low at 1024.\n\nThe server will reach network saturation quickly if this is not dealt\nwith. Further, since we want to optimize for the single Golang\napplication we will give every user on the Linux machine unlimited\nopen files.\n\n# Networking\n\nhttps://engineering.gosquared.com/optimising-nginx-node-js-and-networking-for-heavy-workloads\n\nNewGolangConfig returns the configuration for applications written\nin Go. There is an assumption that the application is going to use\nall the memory on the system as well it being a high throughput\nnetwork application.\n",
  "ProcFS": {
    "net.core.netdev_max_backlog": {
      "Value": "30000",
      "Description": ""
    },
    "net.core.rmem_max": {
      "Value": "16777216",
      "Description": "16MB per socket."
    },
    "net.core.somaxconn": {
      "Value": "16096",
      "Description": "The maximum number of \"backlogged sockets\""
    },
    "net.core.wmem_max": {
      "Value": "16777216",
      "Description": "16MB per socket."
    },
    "net.ipv4.ip_local_port_range": {
      "Value": "1024 65535",
      "Description": "On Linux, the client port has a range of about 30,000 ports. This\nmeans that only 30,000 connections can be established between the\nweb server and the load-balancer every minute, so about 500\nconnections per second. We can increase the amount of available\nports.\n"
    },
    "net.ipv4.tcp_fin_timeout": {
      "Value": "15",
      "Description": "tcp_fin_timeout - INTEGER\nTime to hold socket in state FIN-WAIT-2, if it was closed\nby our side. Peer can be broken and never close its side,\nor even died unexpectedly. Default value is 60sec.\nUsual value used in 2.2 was 180 seconds, you may restore\nit, but remember that if your machine is even underloaded WEB server,\nyou risk to overflow memory with kilotons of dead sockets,\nFIN-WAIT-2 sockets are less dangerous than FIN-WAIT-1,\nbecause they eat maximum 1.5K of memory, but they tend\nto live longer. Cf. tcp_max_orphans.\n"
    },
    "net.ipv4.tcp_max_syn_backlog": {
      "Value": "20480",
      "Description": "Increase the number syn requests allowed."
    },
    "net.ipv4.tcp_max_tw_buckets": {
      "Value": "400000",
      "Description": "Maximal number of timewait sockets held by the system\nsimultaneously. If this number is exceeded time-wait socket\nis immediately destroyed and a warning is printed. This\nlimit exists only to prevent simple DoS attacks, you must\nnot lower the limit artificially, but rather increase it\n(probably, after increasing installed memory), if network\nconditions require more than the default value.\n"
    },
    "net.ipv4.tcp_no_metrics_save": {
      "Value": "1",
      "Description": ""
    },
    "net.ipv4.tcp_rmem": {
      "Value": "4096 87380 67108864",
      "Description": ""
    },
    "net.ipv4.tcp_syn_retries": {
      "Value": "2",
      "Description": ""
    },
    "net.ipv4.tcp_synack_retries": {
      "Value": "2",
      "Description": ""
    },
    "net.ipv4.tcp_syncookies": {
      "Value": "1",
      "Description": ""
    },
    "net.ipv4.tcp_wmem": {
      "Value": "4096 65536 67108864",
      "Description": ""
    },
    "net.netfilter.nf_conntrack_max": {
      "Value": "{{ index \"Vars\" \"nfConntrackMax\" }}",
      "Description": "https://wiki.khnet.info/index.php/Conntrack_tuning\nnf_conntrack. This max should usually double the value of\nthe previous number.\n"
    },
    "proc.file-max": {
      "Value": "2097152",
      "Description": "http://serverfault.com/questions/122679/how-do-ulimit-n-and-proc-sys-fs-file-max-differ\nThis needs to be higher.\n"
    },
    "proc.min_free_kbytes": {
      "Value": "65536",
      "Description": "Amount of memory to keep free. Don't want to make this too high\nas Linux will spend more time trying to reclaim memory.\n"
    }
  },
  "SysFS": {
    "/sys/module/nf_conntrack/parameters/hashsize": {
      "Value": "{{ index \"Vars\" \"nfConntrackMax\" / 4 }}",
      "Description": ""
    }
  },
  "Env": {
    "GOGC": {
      "Value": "2000",
      "Description": "Set the value of GOGC to be really high.\n\nTODO: Consider how this is being used as part of a bigger\nsetting. Based on RAM etc.\n"
    }
  },
  "Vars": {
    "nfConntrackMax": "200000"
  }
}
#+END_SRC

*** ProcFS Changes

 #+begin_src sh :results output code :exports both
 autotune sig --procfs golang
 #+end_src

 #+RESULTS:
 #+BEGIN_SRC sh
 net.ipv4.ip_local_port_range=1024 65535
 net.ipv4.tcp_wmem=4096 65536 67108864
 net.ipv4.tcp_max_syn_backlog=20480
 net.ipv4.tcp_syncookies=1
 net.ipv4.tcp_max_tw_buckets=400000
 net.core.rmem_max=16777216
 proc.file-max=2097152
 proc.min_free_kbytes=65536
 net.ipv4.tcp_syn_retries=2
 net.ipv4.tcp_fin_timeout=15
 net.core.netdev_max_backlog=30000
 net.ipv4.tcp_no_metrics_save=1
 net.ipv4.tcp_rmem=4096 87380 67108864
 net.core.wmem_max=16777216
 net.core.somaxconn=16096
 net.ipv4.tcp_synack_retries=2
 net.netfilter.nf_conntrack_max={{ index "Vars" "nfConntrackMax" }}
 #+END_SRC

*** SysFS Changes

 #+begin_src sh :results output code :exports both
 autotune sig --sysfs golang
 #+end_src

 #+RESULTS:
 #+BEGIN_SRC sh
 /sys/module/nf_conntrack/parameters/hashsize={{ index "Vars" "nfConntrackMax" / 4 }}
 #+END_SRC

*** Environment Variable Changes

 #+begin_src sh :results output code :exports both
 autotune sig --env golang
 #+end_src

 #+RESULTS:
 #+BEGIN_SRC sh
 GOGC=2000
 #+END_SRC


* Agent

The agent allows Acksin to monitor and recommend even more ways of
saving money on your server utilization costs. Agent requires a Fugue
account.

Agent needs to be run as a daemon with an API Key passed to it.

* Open Signatures

#+begin_src ruby :results output drawer :exports results
  require 'json'

  sigs = JSON.parse(`./autotune list`)

  sigs["Open"].each do |s|
    sigInfo = JSON.parse(`./autotune sig --deps #{s}`)

    puts "** #{sigInfo["Name"]}"
    puts
    puts sigInfo["Documentation"]
    puts
    if !sigInfo["ProcFS"].nil? || !sigInfo["SysFS"].nil?
      puts "*** Kernel"
      unless sigInfo["ProcFS"].nil?
        puts
        puts "#+ATTR_HTML: :class table"
        puts "|ProcFS Key|Value|Description|"
        sigInfo["ProcFS"].each do |k, v|
          puts "|=#{k}=|=#{v["Value"]}=|#{v["Description"].gsub("\n", ' ')}|"
        end
      end
      unless sigInfo["SysFS"].nil?
        puts
        puts "#+ATTR_HTML: :class table"
        puts "|SysFS Key|Value|Description|"
        sigInfo["SysFS"].each do |k, v|
          puts "|=#{k}=|=#{v["Value"]}=|#{v["Description"].gsub("\n", ' ')}|"
        end
      end
    end

    if !sigInfo["Env"].nil?
      puts "*** Environment Variables"
      puts
      puts "#+ATTR_HTML: :class table"
      puts "| <10> | <8> ||"
      sigInfo["Env"].each do |k, v|
        puts "|=#{k}=|=#{v["Value"]}=|#{v["Description"].gsub("\n", ' ')}|"
      end
    end

    if !!sigInfo["Deps"] && !sigInfo["Deps"].empty?
      puts "*** Dependencies"
      puts
      sigInfo["Deps"].each do |k|
        puts " - [[#{k}][#{k}]]"
      end
      puts
    end
  end
#+end_src

#+RESULTS:
:RESULTS:
** apache

NewApacheConfig returns the configuration for the Apache HTTP Server.
TODO: Eventually it should be split into apache2-mpm and
apache2-fork.

*** Kernel

#+ATTR_HTML: :class table
|ProcFS Key|Value|Description|
|=kernel.sched_autogroup_enabled=|=0=||
|=kernel.sched_migration_cost_ns=|=5000000=||
*** Dependencies

 - [[networking][networking]]

** golang

Linux Optimizations for High Throughput Golang Apps

Go applications have unique characteristics which require certain
Linux kernel tuning to achieve high throughput.

Go's Utilization Profile

CPU will not be a bottleneck with Golang applications. Our research
shows that applications, even those that utilize CGO, do no see CPU be
a bottleneck. The places where performance become bottlenecks are the
following:

 - Garbage Collection
 - Default ulimits
 - Networking

Assumptions

We will be under the assumption that there will be one primary Go
application running on the machine and can have access to all of the
resources. We also assume that we want high network throughput as the
goal is to have high response rate. We want to be able to handle
millions of requests.

GC Optimizations

For all intents and purposes we should be able to increase the GOGC to
a number based on the size of the machine. If I am using a m4.large
instance on Amazon I use GOGC=10000. The higher the GOGC value the
less frequent the Garbage Collection will run. Further, since we are
optimizing the server to be heavily utilized for a primary Golang
service we want to use up all the RAM available to us.

Ulimits

Ulimits are a security mechanism in POSIX based systems which gives
each user a certain amount of allocation of various
resources. However, the resource we are concerned with is file
descriptors. (ulimit -n) Since a file descriptor can be a file or a
socket we can quickly saturate how many connections an app not running
as root can use. Further, the default open files ulimit on an Ubuntu
Server 14.04 are ridiculously low at 1024.

The server will reach network saturation quickly if this is not dealt
with. Further, since we want to optimize for the single Golang
application we will give every user on the Linux machine unlimited
open files.

# Networking

https://engineering.gosquared.com/optimising-nginx-node-js-and-networking-for-heavy-workloads

NewGolangConfig returns the configuration for applications written
in Go. There is an assumption that the application is going to use
all the memory on the system as well it being a high throughput
network application.

*** Environment Variables

#+ATTR_HTML: :class table
| <10> | <8> ||
|=GOGC=|=2000=|Set the value of GOGC to be really high.  TODO: Consider how this is being used as part of a bigger setting. Based on RAM etc. |
*** Dependencies

 - [[networking][networking]]

** haproxy



*** Dependencies

 - [[networking][networking]]

** java



*** Dependencies

 - [[networking][networking]]

** networking

Many of these settings were from the following places:
  - http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html
  - https://rtcamp.com/tutorials/linux/sysctl-conf/
  - https://fasterdata.es.net/host-tuning/linux/
  - http://cherokee-project.com/doc/other_os_tuning.html
  - https://easyengine.io/tutorials/linux/sysctl-conf/
TODO: These setting are sort of set in stone but I feel that they
can adapt as the system is being used. We don't have to set them to
the values but we can migrate and change as we learn more about the
system and tune it appropriately.

*** Kernel

#+ATTR_HTML: :class table
|ProcFS Key|Value|Description|
|=net.core.netdev_max_backlog=|=30000=||
|=net.core.rmem_max=|=16777216=|16MB per socket.|
|=net.core.somaxconn=|=16096=|The maximum number of "backlogged sockets"|
|=net.core.wmem_max=|=16777216=|16MB per socket.|
|=net.ipv4.ip_local_port_range=|=1024 65535=|On Linux, the client port has a range of about 30,000 ports. This means that only 30,000 connections can be established between the web server and the load-balancer every minute, so about 500 connections per second. We can increase the amount of available ports. |
|=net.ipv4.tcp_fin_timeout=|=15=|tcp_fin_timeout - INTEGER Time to hold socket in state FIN-WAIT-2, if it was closed by our side. Peer can be broken and never close its side, or even died unexpectedly. Default value is 60sec. Usual value used in 2.2 was 180 seconds, you may restore it, but remember that if your machine is even underloaded WEB server, you risk to overflow memory with kilotons of dead sockets, FIN-WAIT-2 sockets are less dangerous than FIN-WAIT-1, because they eat maximum 1.5K of memory, but they tend to live longer. Cf. tcp_max_orphans. |
|=net.ipv4.tcp_max_syn_backlog=|=20480=|Increase the number syn requests allowed.|
|=net.ipv4.tcp_max_tw_buckets=|=400000=|Maximal number of timewait sockets held by the system simultaneously. If this number is exceeded time-wait socket is immediately destroyed and a warning is printed. This limit exists only to prevent simple DoS attacks, you must not lower the limit artificially, but rather increase it (probably, after increasing installed memory), if network conditions require more than the default value. |
|=net.ipv4.tcp_no_metrics_save=|=1=||
|=net.ipv4.tcp_rmem=|=4096 87380 67108864=||
|=net.ipv4.tcp_syn_retries=|=2=||
|=net.ipv4.tcp_synack_retries=|=2=||
|=net.ipv4.tcp_syncookies=|=1=||
|=net.ipv4.tcp_wmem=|=4096 65536 67108864=||
|=net.netfilter.nf_conntrack_max=|={{ index "Vars" "nfConntrackMax" }}=|https://wiki.khnet.info/index.php/Conntrack_tuning nf_conntrack. This max should usually double the value of the previous number. |
|=proc.file-max=|=2097152=|http://serverfault.com/questions/122679/how-do-ulimit-n-and-proc-sys-fs-file-max-differ This needs to be higher. |
|=proc.min_free_kbytes=|=65536=|Amount of memory to keep free. Don't want to make this too high as Linux will spend more time trying to reclaim memory. |

#+ATTR_HTML: :class table
|SysFS Key|Value|Description|
|=/sys/module/nf_conntrack/parameters/hashsize=|={{ index "Vars" "nfConntrackMax" / 4 }}=||
** nginx



*** Dependencies

 - [[networking][networking]]

** nodejs



*** Dependencies

 - [[networking][networking]]

** postgresql

http://www.postgresql.org/message-id/50E4AAB1.9040902@optionshouse.com
http://www.postgresql.org/docs/9.1/static/kernel-resources.html

*** Kernel

#+ATTR_HTML: :class table
|ProcFS Key|Value|Description|
|=kernel.sched_autogroup_enabled=|=0=||
|=kernel.sched_migration_cost_ns=|=5000000=||
|=kernel.shmall=|=4194304=||
|=kernel.shmmax=|=17179869184=||
:END:
